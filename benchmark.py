import time
from typing import List, Dict, Any, Optional
from llm import LLM
from code_executor import CodeExecutor
from mcts import MCTS
from problems import Problems


def run_benchmark(problems: List[Dict], llm: LLM, code_executor: CodeExecutor, 
                  max_iter: int = 16, max_child: int = 3, 
                  verbose: bool = True) -> Dict:
    """
    Executa MCTS em todos os problemas e retorna m√©tricas agregadas
    
    Args:
        problems: Lista de problemas para resolver
        llm: Inst√¢ncia do LLM
        code_executor: Inst√¢ncia do CodeExecutor  
        max_iter: M√°ximo de itera√ß√µes MCTS
        max_child: M√°ximo de filhos por n√≥
        verbose: Se deve mostrar progresso
        
    Returns:
        Dict com resultados individuais e m√©tricas agregadas
    """
    
    results = []
    total_problems = len(problems)
    solved_count = 0
    total_start_time = time.time()
    
    if verbose:
        print(f"üöÄ Iniciando benchmark com {total_problems} problemas")
        print(f"‚öôÔ∏è Configura√ß√£o: max_iter={max_iter}, max_child={max_child}")
        print("-" * 60)
    
    for i, problem in enumerate(problems):
        if verbose:
            print(f"\n[{i+1}/{total_problems}] {problem.get('title', 'Problema desconhecido')}")
        
        try:
            # Cria nova inst√¢ncia MCTS para cada problema
            mcts = MCTS(llm, code_executor, max_iter=max_iter, max_child=max_child)
            
            # Resolve problema
            problem_start_time = time.time()
            solution = mcts.solve(problem)
            problem_time = time.time() - problem_start_time
            
            # Obt√©m m√©tricas
            metrics = mcts.metrics()
            
            # Resultado individual
            result = {
                'problem_id': problem.get('id', f'problem_{i}'),
                'problem_title': problem.get('title', f'Problem {i+1}'),
                'solved': metrics.get('solved', False),
                'solution_code': solution or "",
                'test_success_rate': metrics.get('best_success_rate', 0.0),
                'iterations_used': metrics.get('iterations_used', 0),
                'nodes_explored': metrics.get('nodes_explored', 0),
                'execution_time': problem_time,
                'best_reward': metrics.get('best_reward', 0.0),
                'llm_calls': metrics.get('total_llm_calls', 0)
            }
            
            if result['solved']:
                solved_count += 1
                if verbose:
                    print(f"‚úÖ Resolvido em {result['iterations_used']} itera√ß√µes ({problem_time:.1f}s)")
            else:
                if verbose:
                    success_rate = result['test_success_rate']
                    print(f"‚ùå N√£o resolvido - {success_rate:.1%} dos testes passaram ({problem_time:.1f}s)")
            
            results.append(result)
            
        except Exception as e:
            if verbose:
                print(f"üí• Erro: {e}")
            
            results.append({
                'problem_id': problem.get('id', f'problem_{i}'),
                'problem_title': problem.get('title', f'Problem {i+1}'),
                'solved': False,
                'error': str(e),
                'execution_time': 0.0,
                'test_success_rate': 0.0,
                'iterations_used': 0,
                'nodes_explored': 0,
                'best_reward': -100.0,
                'llm_calls': 0
            })
    
    # Calcula m√©tricas agregadas
    total_time = time.time() - total_start_time
    solved_results = [r for r in results if r.get('solved', False)]
    
    summary = {
        'total_problems': total_problems,
        'solved_problems': solved_count,
        'success_rate': solved_count / total_problems if total_problems > 0 else 0.0,
        'total_execution_time': total_time,
        'avg_time_per_problem': total_time / total_problems if total_problems > 0 else 0.0,
        'avg_iterations_for_solved': sum(r['iterations_used'] for r in solved_results) / len(solved_results) if solved_results else 0.0,
        'avg_nodes_for_solved': sum(r['nodes_explored'] for r in solved_results) / len(solved_results) if solved_results else 0.0,
        'avg_reward_for_solved': sum(r['best_reward'] for r in solved_results) / len(solved_results) if solved_results else 0.0,
        'total_llm_calls': sum(r.get('llm_calls', 0) for r in results),
        'avg_test_success_rate': sum(r.get('test_success_rate', 0) for r in results) / len(results) if results else 0.0
    }
    
    if verbose:
        print_benchmark_summary(summary)
    
    return {
        'individual_results': results,
        'summary': summary,
        'config': {
            'max_iter': max_iter,
            'max_child': max_child,
            'llm_model': llm.model_name
        }
    }


def print_benchmark_summary(summary: Dict):
    """Imprime resumo formatado do benchmark"""
    print("\n" + "="*60)
    print("üìä RESUMO DO BENCHMARK")
    print("="*60)
    print(f"‚úÖ Problemas resolvidos: {summary['solved_problems']}/{summary['total_problems']} ({summary['success_rate']:.1%})")
    print(f"‚è±Ô∏è  Tempo total: {summary['total_execution_time']:.1f}s")
    print(f"‚ö° Tempo m√©dio por problema: {summary['avg_time_per_problem']:.1f}s")
    print(f"üîÑ Itera√ß√µes m√©dias (resolvidos): {summary['avg_iterations_for_solved']:.1f}")
    print(f"üå≥ N√≥s m√©dios explorados (resolvidos): {summary['avg_nodes_for_solved']:.1f}")
    print(f"üéØ Reward m√©dio (resolvidos): {summary['avg_reward_for_solved']:.1f}")
    print(f"ü§ñ Total de chamadas LLM: {summary['total_llm_calls']}")
    print(f"üìà Taxa m√©dia de sucesso em testes: {summary['avg_test_success_rate']:.1%}")


def save_benchmark_results(results: Dict, filename: str = "benchmark_results.json"):
    """Salva resultados do benchmark em arquivo JSON"""
    import json
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"üíæ Resultados salvos em {filename}")
    except Exception as e:
        print(f"‚ùå Erro ao salvar resultados: {e}")


def run_single_problem_benchmark(problem_id: str, problems_file: str = "leetcode_problems.json",
                                max_iter: int = 16, max_child: int = 3) -> Dict:
    """Executa benchmark em um √∫nico problema espec√≠fico"""
    
    # Carrega problemas
    problems = Problems()
    all_problems = problems.load_json(problems_file)
    
    # Encontra problema espec√≠fico
    target_problem = None
    for p in all_problems:
        if p.get('id') == problem_id:
            target_problem = p
            break
    
    if not target_problem:
        raise ValueError(f"Problema '{problem_id}' n√£o encontrado")
    
    # Configura componentes
    llm = LLM()
    code_executor = CodeExecutor()
    
    # Executa benchmark
    return run_benchmark([target_problem], llm, code_executor, max_iter, max_child) 