#!/usr/bin/env python3
"""
Script de teste para verificar se todo o sistema MCTS est√° funcionando
"""

import traceback
from llm import LLM
from code_executor import CodeExecutor
from problems import Problems
from mcts import MCTS
from benchmark import run_benchmark, run_single_problem_benchmark, save_benchmark_results


def test_components():
    """Testa componentes individuais"""
    print("üß™ TESTANDO COMPONENTES INDIVIDUAIS")
    print("="*50)
    
    # Teste 1: PromptManager
    print("\n1Ô∏è‚É£ Testando PromptManager...")
    try:
        from prompt_manager import PromptManager
        pm = PromptManager()
        
        # Testa modifica√ß√£o de prompt
        pm.add_before('get_weak_answer', "You are an expert programmer. ")
        prompt = pm.get_prompt('get_weak_answer', question="Test question", answer_format="Python")
        print(f"‚úÖ PromptManager OK - Prompt gerado: {len(prompt)} chars")
        
    except Exception as e:
        print(f"‚ùå PromptManager FALHOU: {e}")
        return False
    
    # Teste 2: CodeExecutor
    print("\n2Ô∏è‚É£ Testando CodeExecutor...")
    try:
        executor = CodeExecutor()
        
        # Testa extra√ß√£o de c√≥digo
        sample_response = """
        Here's the solution:
        ```python
        def test_func(x):
            return x * 2
        ```
        """
        code = executor.extract_code(sample_response)
        print(f"‚úÖ CodeExecutor extra√ß√£o OK - C√≥digo: {code[:50]}...")
        
        # Testa valida√ß√£o de sintaxe
        valid, error = executor.validate_syntax(code)
        print(f"‚úÖ CodeExecutor valida√ß√£o OK - V√°lido: {valid}")
        
    except Exception as e:
        print(f"‚ùå CodeExecutor FALHOU: {e}")
        return False
    
    # Teste 3: Problems
    print("\n3Ô∏è‚É£ Testando Problems...")
    try:
        problems = Problems()
        all_problems = problems.load_json("leetcode_problems.json")
        print(f"‚úÖ Problems OK - {len(all_problems)} problemas carregados")
        
        # Testa acesso por √≠ndice
        first_problem = problems[0]
        print(f"‚úÖ Primeiro problema: {first_problem.get('title', 'Sem t√≠tulo')}")
        
    except Exception as e:
        print(f"‚ùå Problems FALHOU: {e}")
        return False
    
    # Teste 4: LLM (sem chamar modelo real)
    print("\n4Ô∏è‚É£ Testando LLM (setup apenas)...")
    try:
        llm = LLM()
        print(f"‚úÖ LLM OK - Modelo: {llm.model_name}")
        print(f"‚úÖ Prompts dispon√≠veis: {len(llm.prompts.list_prompts())}")
        
    except Exception as e:
        print(f"‚ùå LLM FALHOU: {e}")
        return False
    
    print("\n‚úÖ Todos os componentes b√°sicos OK!")
    return True


def test_integration():
    """Testa integra√ß√£o entre componentes"""
    print("\nüîó TESTANDO INTEGRA√á√ÉO")
    print("="*50)
    
    try:
        # Configura componentes
        llm = LLM()
        code_executor = CodeExecutor()
        problems = Problems()
        all_problems = problems.load_json("leetcode_problems.json")
        
        # Testa cria√ß√£o do MCTS
        mcts = MCTS(llm, code_executor, max_iter=2, max_child=2)  # Limites baixos para teste
        print("‚úÖ MCTS criado com sucesso")
        
        # Testa modo debug (sem executar)
        first_problem = all_problems[0]
        print(f"‚úÖ Problema selecionado: {first_problem.get('title')}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Integra√ß√£o FALHOU: {e}")
        traceback.print_exc()
        return False


def test_single_problem():
    """Testa execu√ß√£o em um √∫nico problema (modo debug)"""
    print("\nüéØ TESTANDO PROBLEMA √öNICO (MODO DEBUG)")
    print("="*50)
    
    try:
        # Configura
        llm = LLM()
        code_executor = CodeExecutor()
        problems = Problems()
        all_problems = problems.load_json("leetcode_problems.json")
        
        # Seleciona problema mais simples
        test_problem = all_problems[0]  # isPalindrome
        print(f"üéØ Testando: {test_problem.get('title')}")
        
        # Cria MCTS com limites baixos
        mcts = MCTS(llm, code_executor, max_iter=3, max_child=2)
        
        # Entra em modo debug
        mcts.debug(test_problem)
        print("‚úÖ Modo debug ativado")
        
        # Executa alguns steps
        print("\nüîÑ Executando steps...")
        for i in range(2):  # Apenas 2 steps para teste
            if mcts.step():
                print(f"Step {i+1} executado")
            else:
                print("Execu√ß√£o parou (solu√ß√£o encontrada ou erro)")
                break
        
        # Verifica m√©tricas
        metrics = mcts.metrics()
        print(f"‚úÖ M√©tricas obtidas: {len(metrics)} campos")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Teste problema √∫nico FALHOU: {e}")
        traceback.print_exc()
        return False


def test_benchmark_small():
    """Testa benchmark com poucos problemas"""
    print("\nüìä TESTANDO BENCHMARK PEQUENO")
    print("="*50)
    
    try:
        # Configura
        llm = LLM()
        code_executor = CodeExecutor()
        problems = Problems()
        all_problems = problems.load_json("leetcode_problems.json")
        
        # Seleciona apenas 2 primeiros problemas
        test_problems = all_problems[:2]
        print(f"üéØ Testando benchmark com {len(test_problems)} problemas")
        
        # Executa benchmark com limites baixos
        results = run_benchmark(
            problems=test_problems,
            llm=llm,
            code_executor=code_executor,
            max_iter=3,  # Poucos itera√ß√µes para teste
            max_child=2,
            verbose=True
        )
        
        print(f"‚úÖ Benchmark executado!")
        print(f"‚úÖ Resultados: {len(results['individual_results'])} problemas")
        print(f"‚úÖ Taxa de sucesso: {results['summary']['success_rate']:.1%}")
        
        # Salva resultados
        save_benchmark_results(results, "test_results.json")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Benchmark FALHOU: {e}")
        traceback.print_exc()
        return False


def main():
    """Executa todos os testes"""
    print("üöÄ INICIANDO TESTES DO SISTEMA MCTS")
    print("="*60)
    
    tests = [
        ("Componentes", test_components),
        ("Integra√ß√£o", test_integration),
        ("Problema √∫nico", test_single_problem),
        ("Benchmark pequeno", test_benchmark_small)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name.upper()} {'='*20}")
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"‚ùå ERRO CR√çTICO em {test_name}: {e}")
            results.append((test_name, False))
    
    # Resumo final
    print(f"\n{'='*60}")
    print("üìã RESUMO DOS TESTES")
    print("="*60)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASSOU" if success else "‚ùå FALHOU"
        print(f"{test_name:20} {status}")
    
    print(f"\nüéØ RESULTADO FINAL: {passed}/{total} testes passaram ({passed/total:.1%})")
    
    if passed == total:
        print("üéâ TODOS OS TESTES PASSARAM! Sistema pronto para uso.")
    else:
        print("‚ö†Ô∏è  Alguns testes falharam. Verifique os erros acima.")


if __name__ == "__main__":
    main() 